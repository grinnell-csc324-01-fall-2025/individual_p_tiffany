# Interview Transcript:

## Introduction
This is the organized transcript of an interview discussing expectations and concerns regarding AI-based therapy.  
The interviewer is marked as **Me**, and the participant is marked as **Interviewee**.


## Interview 1

**Me:** Could you tell me about your initial thoughts on using AI therapy?  

**Interviewee 1:** My first thought is that an AI therapist might be able to provide very comprehensive responses because of the vast amount of information it can draw on. It could take into account many different perspectives simultaneously. But at the same time, it may struggle to fully grasp the specifics of feelings, tones, or subtle human dynamics. That makes it hard for AI to give advice truly tailored to an individual.  

**Me:** So, you feel it may be difficult for AI to capture subtle emotions in human descriptions. Is that correct?  

**Interviewee 1:** Yes, that’s part of it. The advice might end up being too broad. To give meaningful support, AI would need to gather a lot of personal data—perhaps through “getting-to-know-you” sessions, where someone can share their backstory.  

**Me:** That’s a good point. What concerns would you have about sharing personal information with an AI therapy product?  

**Interviewee 1:** If confidentiality is ensured, I would be comfortable. My main concern is misuse, such as the AI using personal disclosures for marketing. For example, if I shared sadness about lacking something, I wouldn’t want to be targeted with ads exploiting that insecurity. As long as the data remains private and only used internally for improving the service, I would feel okay.  

**Me:** So you’d expect explicit consent agreements—like clear confidentiality forms—before starting.  

**Interviewee 1:** Absolutely. I’d want to know exactly how my information is collected, used, and protected.  

**Me:** Let’s imagine you are using an AI therapy platform on your computer. What design or features would make you feel comfortable and supported?  

**Interviewee 1:** I’d want visuals that convey calmness—neutral or cream colors rather than stark white, and soothing fonts. Personalization would be key: the ability to design a “therapy office” space, choose themes, ambient sounds (rain, clouds, jungle, mountains), and even a mode like “walk during session.” Functionally, I’d like a tabbed structure:  
- General conversation  
- Positive moments or daily highlights  
- Journal for free writing  
- Quick targeted advice  
- A dedicated therapy session mode  

I’d prefer voice recording rather than video, since video feels too vulnerable, especially if I were emotional. Consent prompts should be phrased in a calming, collaborative way—not as if I’m being monitored, but as if the system is trying to understand me.  

**Me:** That makes sense. Customization seems very important to you.  

**Interviewee 1:** Yes. I’d love to customize the therapy space, almost like designing a home in a game such as *Animal Crossing*. This would create comfort and a sense of control. Gentle prompts would also help—reminders to drink water, get comfortable, or practice self-kindness.  

**Me:** On a scale from one to ten, how important is privacy and data security to you when considering AI therapy?  

**Interviewee 1:** A ten. I would always choose a secure product over a non-secure one, even if the interface were less appealing. Confidentiality is a prerequisite; without it, I wouldn’t use the product at all.  

**Me:** Besides privacy, what other factors would motivate you to try AI therapy instead of human counseling?  

**Interviewee 1:** Traditional therapy sometimes left me feeling misunderstood, or sessions ended before I could fully express myself. AI could improve this by letting me select my intent beforehand: whether I just want space to talk, need advice, or want a mix. Having the flexibility to shift during a session without social pressure would be powerful. Cost and accessibility are also big motivators—AI therapy would be much more appealing if it’s significantly more affordable and easier to access.  

**Me:** Would affordability be a top-three factor in your decision?  

**Interviewee 1:** Definitely. I’d be more willing to try AI therapy first, because seeing a human therapist involves higher costs, logistics, and barriers. I’d rate my openness to AI therapy around nine out of ten, compared to about five out of ten for traditional therapy due to those barriers.  

**Me:** That’s very insightful. Do you have any questions for me?  

**Interviewee 1:** I wonder whether an AI could learn when to encourage someone to take action versus when to simply hold space. For example, if I were feeling sluggish and needed rest, would the AI recognize that? Or would it know when to gently encourage me to get up, hydrate, or journal? Striking that balance is important.  

**Me:** That’s an excellent question. In cognitive behavioral therapy, for example, the focus is first on understanding a person’s thoughts and emotions. Once people feel supported, they naturally become more motivated to take helpful actions. AI should function more as an emotional supporter, guiding users to their own intrinsic motivation rather than giving direct instructions.  

**Interviewee 1:** That makes sense. One more question: how would the AI handle confidentiality while also being prepared to intervene in crises, such as suicidal thoughts?  

**Me:** That’s essential to include in the agreement forms, just like in human counseling. Users must be informed upfront that confidentiality is respected, except in cases of suicidality or imminent harm. This ensures both user safety and community responsibility.  

**Interviewee 1:** Thank you. I’m glad I could contribute my thoughts.  

**Me:** Thank you as well—I truly appreciate your insights.  
